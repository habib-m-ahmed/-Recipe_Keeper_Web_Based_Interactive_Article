<!DOCTYPE html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=650, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <script type="text/javascript" src="assets/dynamic_visuals/utils.js"></script>
  <script type="text/javascript" src="assets/dynamic_visuals/globals.js"></script>

  <!-- - Small mathjax addition to support equation numbering. -->

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
  </script>

  <script async type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

  <style id="distill-manual-toc">
    /* TOC */
    #contents nav a {
      color: rgba(0, 0, 0, 0.8);
      border-bottom: none;
      text-decoration: none;
    }

    #contents nav ul li {
      margin-bottom: .25em;
    }

    #contents nav a:hover {
      text-decoration: underline solid rgba(0, 0, 0, 0.6);
    }

    #contents nav ul {
      margin-top: 0;
      margin-bottom: 6px;
    }


    #contents nav>div {
      display: block;
      outline: none;
      margin-bottom: 0.5em;
    }

    #contents nav>div>a {
      font-size: 13px;
      font-weight: 600;
    }
  </style>
  <style>
    #model_table>table,
    th,
    td {
      border: 1px solid #dfe2e5;
      border-collapse: collapse;
      padding: 6px 13px !important;
    }

    tr:nth-child(even)>td {
      background-color: rgb(246, 248, 250);
    }
  </style>

  <link rel="stylesheet" type="text/css" href="assets/dynamic_visuals/style.css">
</head>

<body>
  <!--
  
  <d-front-matter>
   

    <script id="distill-front-matter" type="text/json">{
      "title": "Recipe Keeper - Group 6 Final Report",
      "description": "Our application is a redesign of the Recipe Keeper app, which is an all-in-one recipe organizer, shopping list, and meal planner available across devices (desktop, mobile and tablet). The app allows users to store all their recipes in one place, so that they can better plan their meals and grocery needs.",
      "authors": [
        {"author": "Anjana Somasundaram"},
        {"author": "Sajin Kowser"},
        {"author": "Aly Dayani"},
        {"author": "Eric Wang"},
        {"author": "Habib Ahmed"}
      ]
    }</script> 

  </d-front-matter>

  -->

  <d-title>
    <h1>Recipe Keeper - Group 6 Final Report</h1>
    <p style="grid-column: text;">Our application is a redesign of the Recipe Keeper app, which is an all-in-one recipe organizer, shopping list, and meal planner available across devices (desktop, mobile and tablet). The app allows users to store all their recipes in one place, so that they can better plan their meals and grocery needs.</p>
  </d-title>

  <!-- TO ENTER HORIZONTAL LINE THAT SPLITS SECTIONS: 
      <d-byline></d-byline>
  -->

  <div id="contents" class="base-grid" style="border-top: 1px solid #eee; padding: 1.5rem 0;">
    <nav class="l-text toc figcaption">
      <h3>Contents</h3>
      <div><a href="#executive_summary">Executive summary</a></div>
      <div><a href="#navigational_map">Navigational map</a></div>
      <div><a href="#design_principles">Design principles</a></div>
      <!--To add indented bullet points as sub-topics:
      <ul>
        <li><a href="#computing-receptive-field-size">Computing receptive field size</a></li>
        <li><a href="#computing-receptive-field-region">Computing receptive field region in input image</a></li>
      </ul>
      -->
      <div><a href="#final_heuristic_evaluation">Final heuristic evaluation</a></div>
      <div><a href="#discussion">Discussion: recommendations to improve the system</a></div>
    </nav>
  </div>


  <d-article>

    <p>
      As highlighted in our initial proposal, the app’s tagline is to “Cook more. Eat Healthier. Shop Smarter.” Our design approach focuses on considering key parts of the user experience and implementing / improving features that help it better achieve that promise.

      <!--
      <video width="640" height="480" controls>
        <source src="HCI Presentation.mp4" type="video/mp4">
      Your browser does not support the video tag.
      </video>
      -->

    </p>

    <p>
      VIDEO PRESENTATION PLACEHOLDER
    </p>

    <h2 id="executive_summary">Executive Summary</h2>

    <p>
      PLACEHOLDER (waiting on Aly for new executive summary)
    </p>

    <p>
      Our redesigned Recipe Keeper app has been created to store and process the same information as the original app, but in a manner that ensures an ideal user experience through more suitable cognitive effects. To achieve this, our design process focused heavily on how to represent the information and build interactions that aligned with the users' needs. Our design approach was based on the representation design framework that considered user tasks, underlying information, cognitive abilities, and context of use.
    </p>

    <p>
      To address user tasks such as comparison, adding to planner, and viewing recipes, our redesign focuses on intuitive interactions that are easy to use and help users to achieve their goals efficiently. For example, our recipe comparison feature uses a side-by-side layout that enables users to compare two recipes quickly. Additionally, our meal planner feature allows users to add recipes into their calendar, making it easier to plan and organize meals.
    </p>

    <p>
      When considering the underlying information such as quantitative, qualitative, and text data, our redesign was focused on presenting the data in an easy-to-understand manner. For example, within the comparison feature, when a user is comparing the nutritional information of two different recipes, as the information is quantitative, the representation is able to highlight the “better” recipe based on the statistic. 
    </p>

    <p>
      Our redesign also takes into account the cognitive abilities and limitations of users while using the app. For instance, while cooking, users may not be able to read instructions or interact with the app using their hands. To address this, we have included “Sous Chef”, a voice and audio feature that allows users to access recipes using voice commands.
    </p>

    <p>
      Finally, our redesign considers the context of use, which includes being organized with recipes, wanting to cook more, and being informed. Accordingly, we have included a search feature that makes it easy for users to find recipes based on their dietary requirements or preferences. This was also the inspiration for a meal recommender feature, where showing the user the picture of a delicious recipe that they could make would encourage them to not only cook but also use other aspects of the app.
    </p>


    <h2 id="navigational_map">Navigational Map</h2>

    <img src="image2.png" alt="Navigational Map">

    <p></p>

    <p>
      <b>
        Description of Screens:
      </b>
    </p>

    <p>
      <i>
      The following screens have been outlined in the order in which the prototype works (click each image to enlarge).
      </i>
    </p>

    <style>
      /* The grid: Four equal columns that floats next to each other */
  .column {
    float: left;
    width: 20%;
    padding: 10px;
  }
  
  /* Style the images inside the grid */
  .column img {
    opacity: 0.8;
    cursor: pointer;
  }
  
  .column img:hover {
    opacity: 1;
  }
  
  /* Clear floats after the columns */
  .row:after {
    content: "";
    display: table;
    clear: both;
  }
  
  /* The expanding image container (positioning is needed to position the close button and the text) */
  .container {
    position: relative;
    display: none;
  }
  
  /* Expanding image text */
  #imgtext {
    position: absolute;
    bottom: 8px;
    right: 16px;
    color: #000000;
    font-size: 25px;
  }
  
  /* Closable button inside the image */
  .closebtn {
    position: absolute;
    top: 10px;
    right: 15px;
    color: rgb(0, 0, 0);
    font-size: 40px;
    cursor: pointer;
  }
    </style>

    <!-- The grid: four columns -->
    <div class="row">
      <div class="column">
        <img src="image5.png" alt="Home" style="width:150px;height:100px;" onclick="myFunction(this);">
      </div>
      <div class="column">
        <img src="image6.png" alt="Search" style="width:150px;height:100px;" onclick="myFunction(this);">
      </div>
      <div class="column">
        <img src="image4.jpg" alt="Recipe Comparer" style="width:150px;height:100px;" onclick="myFunction(this);">
      </div>
      <div class="column">
        <img src="image3.png" alt="Meal Planner" style="width:150px;height:100px;" onclick="myFunction(this);">
      </div>
      <div class="column">
        <img src="image7.png" alt="Recommender" style="width:150px;height:100px;" onclick="myFunction(this);">
      </div>
      <div class="column">
        <img src="image5.png" alt="Bulk Add" style="width:150px;height:100px;" onclick="myFunction(this);">
      </div>
      <div class="column">
        <img src="image3.png" alt="Recipe Scanner" style="width:150px;height:100px;" onclick="myFunction(this);">
      </div>
      <div class="column">
        <img src="image1.png" alt="Sous Chef Mode" style="width:150px;height:100px;" onclick="myFunction(this);">
      </div>
    </div>

<!-- The expanding image container -->
<div class="container">
  <!-- Close the image -->
  <span onclick="this.parentElement.style.display='none'" class="closebtn">&times;</span>

  <!-- Expanded image -->
  <img id="expandedImg" style="width:100%">

  <!-- Image text -->
  <div id="imgtext"></div>
</div>

  <script>
  function myFunction(imgs) {
    var expandImg = document.getElementById("expandedImg");
    var imgText = document.getElementById("imgtext");
    expandImg.src = imgs.src;
    imgText.innerHTML = imgs.alt;
    expandImg.parentElement.style.display = "block";
  }
  </script>


  <h3>Home:</h3>
  
    The home screen features all recipes. Users can click into the “Main Dish” quadrant to access the recipe comparer feature. 
  
  <h3>Search:</h3>

    Users can click on “Advanced Search” to access this functionality. They are then able to filter recipes based on words, preparation, and/or nutritional information using the accordion style menu. 
 
  <h3>Recipe Comparer:</h3>

    Users can drag and drop two recipes to compare their nutritional makeup in a side-by-side fashion.

  <h3>Meal Planner:</h3>

    Users can then click the “Meal Planner” button on the left navigation tab to access this feature. They can press the “+ Add Meal”' button to manually add meals to the weekly planner. This involves selecting the date, meal type, and recipe. Alternatively, they can press and hold the spacebar to auto-populate the entire week with meals based on the user’s preference. Note, for the purposes of the Figma, this interaction entails clicking a space bar (vs. holding it down).

  <h3>Recommender:</h3>

    When clicking back into Home, the user is served a recommendation based on the time of day and their eating preferences. They can choose to view the recipe or hit cancel.

  <h3>Bulk Add:</h3>

    Users can select the “+ New Recipe” option from the left navigation bar, and choose “Import recipe from website” to access this feature. They are then able to bulk add multiple recipes (at one time) by clicking on the “+” button to input more links.

  <h3>Recipe Scanner:</h3>

    To access the recipe scanner, the user must select the “Shopping List” option on the left navigation bar. They are then able to upload a photo. If the photo fails to upload, they are encouraged to retry the operation. Once the image has been successfully uploaded, they can click into it in order to identify the different ingredients. They are then able to select/tick these ingredients and hit “add” to populate their shopping list with the chosen items.

  <h3>Sous Chef Mode:</h3>

    Users can press the “Sous Chef Mode” option on the left navigation bar to access the voice assistant. They are then able to walk through a sample recipe from start to finish. Note, for the purposes of the Figma, the voice commands are not operational and are purely illustrative.

    <iframe style="border: 1px solid rgba(0, 0, 0, 0.1);" width="800" height="450" src="https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FnHXPKbDTRXIHypoJJGfS3C%2FStoryboard%3Fnode-id%3D400%253A31762%26t%3DDRTJ4lVb4dSkPUuz-1" allowfullscreen></iframe>

    <h2 id="design_principles">20 Design Principles</h2>

    <p>
      In this section, we compute recurrence and closed-form expressions for
      fully-convolutional networks with a single path from input to output
      (e.g.,
      AlexNet <d-cite key="krizhevsky2012imagenet"></d-cite>
      or
      VGG <d-cite key="simonyan2015very"></d-cite>).
    </p>

    <h3 id="computing-receptive-field-size">Computing receptive field size</h3>

    <p>
      Define \(r_l\) as the receptive field size of
      the final output feature map \(f_{L}\), with respect to feature map \(f_l\). In
      other words, \(r_l\) corresponds to the number of features in feature map
      \(f_l\) which contribute to generate one feature in \(f_{L}\). Note
      that \(r_{L}=1\).
    </p>

    <p>
      As a simple example, consider layer \(L\), which takes features \(f_{L-1}\) as
      input, and generates \(f_{L}\) as output. Here is an illustration:
    </p>

    <figure class="figure-contents" style="height: 200px">

      <svg id="fig2b_svg" height="320" width="500" style="position: relative; top: -100px;">
        Sorry, your browser does not support inline SVG.
      </svg>
      
      <div id="fig2b_params_l1" style="position: absolute; top: 83px; right: 0px; text-align: right;">
      <div style="height: 20px;">Kernel Size (k<sub>L</sub>): <span>2</span></div>
      <div style="height: 20px;">Padding (p<sub>L</sub>, q<sub>L</sub>): <span>0</span></div>
      <div style="height: 20px;">Stride (s<sub>L</sub>): <span>3</span></div>
      </div>
      <div style="position: absolute; top: -20px; left: 115px;">k<sub>L</sub></div>
      <div style="position: absolute; top: -20px; left: 200px;">k<sub>L</sub></div>
      <div id="fig2b_f0" style="position: absolute; top: 40px; left: 40px;">f<sub>L-1</sub></div>
      <div id="fig2b_f1" style="position: absolute; top: 170px; left: 40px;">f<sub>L</sub></div>

      <script type="text/javascript" src="assets/dynamic_visuals/fig2b.js">
      </script>

    </figure>

    <p>
      It is easy to see that \(k_{L}\)
      features from \(f_{L-1}\) can influence one feature from \(f_{L}\), since each
      feature from \(f_{L}\) is directly connected to \(k_{L}\) features from
      \(f_{L-1}\). So, \(r_{L-1} = k_{L}\).
    </p>

    <p>
      Now, consider the more general case where we know \(r_{l}\) and want to compute
      \(r_{l-1}\). Each feature \(f_{l}\) is connected to \(k_{l}\) features from
      \(f_{l-1}\).
    </p>

    <p>
      First, consider the situation where \(k_l=1\): in this case, the \(r_{l}\)
      features in \(f_{l}\) will cover \(r_{l-1}=s_l\cdot r_{l} - (s_l - 1)\) features
      in in \(f_{l-1}\). This is illustrated in the figure below, where \(r_{l}=2\)
      (highlighted in red). The first term \(s_l \cdot r_{l}\) (green) covers the
      entire region where the
      features come from, but it will cover \(s_l - 1\) too many features (purple),
      which is why it needs to be deducted.
      <d-footnote>
        As in the illustration below, note that, in some cases, the receptive
        field region may contain "holes", i.e., some of the input features may be
        unused for a given layer.
      </d-footnote>
    </p>

    <figure class="figure-contents" style="height: 280px">

      <svg id="fig2_svg" height="350" width="500" style="position: relative; top: -70px;">
        Sorry, your browser does not support inline SVG.
      </svg>
      
      <div id="fig2_params_l1" style="position: absolute; top: 113px; right: 0px; text-align: right;">
      <div style="height: 20px;">Kernel Size (k<sub>l</sub>): <span>1</span></div>
      <div style="height: 20px;">Padding (p<sub>l</sub>, q<sub>l</sub>): <span>0</span></div>
      <div style="height: 20px;">Stride (s<sub>l</sub>): <span>3</span></div>
      </div>
      <div style="position: absolute; top: 10px; left: 130px;">s<sub>l</sub>⋅r<sub>l</sub></div>
      <div style="position: absolute; top: -5px; left: 228px;">s<sub>l</sub>-1</div>
      <div id="fig2_f0" style="position: absolute; top: 70px; left: 40px;">f<sub>l-1</sub></div>
      <div id="fig2_f1" style="position: absolute; top: 200px; left: 40px;">f<sub>l</sub></div>
      <div style="position: absolute; top: 255px; left: 170px;">r<sub>l</sub>=2</div>
      
      <script type="text/javascript" src="assets/dynamic_visuals/fig2.js">
      </script>

    </figure>

    <p>
      For the case where \(k_l > 1\), we just need to add \(k_l-1\) features, which
      will cover those from the left and the right of the region. For example, if we
      use a kernel size of \(5\) (\(k_l=5\)), there would be \(2\) extra features used
      on each side, adding \(4\) in total. If \(k_l\) is even, this works as well,
      since the left and right padding will add to \(k_l-1\).
      <d-footnote>
        Due to border effects, note that the size of the region in the original
        image which is used to compute each output feature may be different. This
        happens if padding is used, in which case the receptive field for
        border features includes the padded region. Later in the article, we
        discuss how to compute the receptive field region for each feature,
        which can be used to determine exactly which image pixels are used for
        each output feature.
      </d-footnote>
    </p>

    <figure class="figure-contents" style="height: 280px">

      <svg id="fig2c_svg" height="350" width="800" style="position: relative; top: -60px;">
        Sorry, your browser does not support inline SVG.
      </svg>
      
      <div id="fig2c_params_l1" style="position: absolute; top: 113px; right: 0px; text-align: right;">
      <div style="height: 20px;">Kernel Size (k<sub>l</sub>): <span>5</span></div>
      <div style="height: 20px;">Padding (p<sub>l</sub>, q<sub>l</sub>): <span>0</span></div>
      <div style="height: 20px;">Stride (s<sub>l</sub>): <span>3</span></div>
      </div>
      <div style="position: absolute; top: 20px; left: 175px;">s<sub>l</sub>⋅r<sub>l</sub> - (s<sub>l</sub>-1)</div>
      <div style="position: absolute; top: 5px; left: 275px;">(k<sub>l</sub>-1)/2</div>
      <div style="position: absolute; top: 5px; left: 95px;">(k<sub>l</sub>-1)/2</div>
      <div id="fig2c_f0" style="position: absolute; top: 80px; left: 40px;">f<sub>l-1</sub></div>
      <div id="fig2c_f1" style="position: absolute; top: 210px; left: 40px;">f<sub>l</sub></div>
      <div style="position: absolute; top: 265px; left: 200px;">r<sub>l</sub>=2</div>

      <script type="text/javascript" src="assets/dynamic_visuals/fig2c.js">
      </script>

    </figure>

    <p>
      So, we obtain the general recurrence equation (which is
      <a
        href="https://en.wikipedia.org/wiki/Recurrence_relation#Solving_first-order_non-homogeneous_recurrence_relations_with_variable_coefficients">
        first-order,
        non-homogeneous, with variable
        coefficients
      </a>
      ):
    </p>

    <p>
      \(\begin{align}
      r_{l-1} = s_l \cdot r_{l} + (k_l - s_l)
      \label{eq:rf_recurrence}\
      \end{align}\)
    </p>

    <p>
      This equation can be used in a recursive algorithm to compute the receptive
      field size of the network, \(r_0\). However, we can do even better: we can <a href="#solving-receptive-field-size" id="return-from-solving-receptive-field-size">solve
      the recurrence equation</a> and obtain a solution in terms of the \(k_l\)'s and
      \(s_l\)'s:
    </p>

    <p>
      \begin{equation}
      r_0 = \sum_{l=1}^{L} \left((k_l-1)\prod_{i=1}^{l-1}
      s_i\right) + 1 \label{eq:rf_recurrence_final} \end{equation}
    </p>

    <p>
      This expression makes intuitive sense, which can be seen by considering some
      special cases. For example, if all kernels are of size 1, naturally the
      receptive field is also of size 1. If all strides are 1, then the receptive
      field will simply be the sum of \((k_l-1)\) over all layers, plus 1, which is
      simple to see. If the stride is greater than 1 for a particular layer, the region
      increases proportionally for all layers below that one. Finally, note that
      padding does not need to be taken into account for this derivation.
    </p>

    <h3 id="computing-receptive-field-region">Computing receptive field region in input image</h3>

    <p>
      While it is important to know the size of the region which generates one feature
      in the output feature map, in many cases it is also critical to precisely
      localize the region which generated a feature. For example, given feature
      \(f_{L}(i, j)\), what is the region in the input image which generated it? This
      is addressed in this section.
    </p>

    <p>
      Let's denote \(u_l\) and \(v_l\) the left-most
      and right-most coordinates (in \(f_l\)) of the region which is used to compute the
      desired feature in \(f_{L}\). In these derivations, the coordinates are zero-indexed (i.e., the first feature in
      each map is at coordinate \(0\)).

      Note that \(u_{L} = v_{L}\) corresponds to the
      location of the desired feature in \(f_{L}\). The figure below illustrates a
      simple 2-layer network, where we highlight the region in \(f_0\) which is used
      to compute the first feature from \(f_2\). Note that in this case the region
      includes some padding. In this example, \(u_2=v_2=0\), \(u_1=0,v_1=1\), and
      \(u_0=-1, v_0=4\).
    </p>

    <figure class="figure-contents">

      <svg id="fig3_svg" height="450" width="500" style="position: relative; top: -20px;">
        Sorry, your browser does not support inline SVG.
      </svg>
      
      <div id="fig3_params_l1" style="position: absolute; top: 163px; right: 0px; text-align: right;">
      <div style="height: 20px;">Kernel Size (k<sub>1</sub>): <span>3</span></div>
      <div style="height: 20px;">Padding (p<sub>1</sub>, q<sub>1</sub>): <span>1</span></div>
      <div style="height: 20px;">Stride (s<sub>1</sub>): <span>3</span></div>
      </div>
      <div id="fig3_params_l2" style="position: absolute; top: 293px; right: 0px; text-align: right;">
      <div style="height: 20px;">Kernel Size (k<sub>2</sub>): <span>2</span></div>
      <div style="height: 20px;">Padding (p<sub>2</sub>, q<sub>2</sub>): <span>0</span></div>
      <div style="height: 20px;">Stride (s<sub>2</sub>): <span>1</span></div>
      </div>
      <div style="position: absolute; top: 35px; left: 53px;">u<sub>0</sub> = -1</div>
      <div style="position: absolute; top: 35px; left: 203px;">v<sub>0</sub> = 4</div>
      <div id="fig3_f0" style="position: absolute; top: 120px; left: 40px;">f<sub>0</sub></div>
      <div id="fig3_f1" style="position: absolute; top: 250px; left: 40px;">f<sub>1</sub></div>
      <div id="fig3_f2" style="position: absolute; top: 380px; left: 40px;">f<sub>2</sub></div>
      
      <script type="text/javascript" src="assets/dynamic_visuals/fig3.js">
      </script>

    </figure>

    <p>
      We'll start by asking the following question: given \(u_{l}, v_{l}\), can we
      compute \(u_{l-1},v_{l-1}\)?
    </p>

    <p>
      Start with a simple case: let's say \(u_{l}=0\) (this corresponds to the first
      position in \(f_{l}\)). In this case, the left-most feature \(u_{l-1}\) will
      clearly be located at \(-p_l\), since the first feature will be generated by
      placing the left end of the kernel over that position. If \(u_{l}=1\), we're
      interested in the second feature, whose left-most position \(u_{l-1}\) is \(-p_l
      + s_l\); for \(u_{l}=2\), \(u_{l-1}=-p_l + 2\cdot s_l\); and so on. In general:
    </p>

    <p>
      \(\begin{align}
      u_{l-1}&amp;= -p_l + u_{l}\cdot s_l \label{eq:rf_loc_recurrence_u} \\
      v_{l-1}&amp;= -p_l + v_{l}\cdot s_l + k_l -1
      \label{eq:rf_loc_recurrence_v}
      \end{align}\)
    </p>

    <p>
      where the computation of \(v_l\) differs only by adding \(k_l-1\), which is
      needed since in this case we want to find the right-most position.
    </p>

    <p>
      Note that these expressions are very similar to the recursion derived for the
      receptive field size \eqref{eq:rf_recurrence}. Again, we could implement a
      recursion over the network to obtain \(u_l,v_l\) for each layer; but we can also
      <a href="#solving-receptive-field-region" id="return-from-solving-receptive-field-region">solve for \(u_0,v_0\)</a> and obtain closed-form expressions in terms of the
      network parameters:
    </p>

    <p>
      \(\begin{align}
      u_0&amp;= u_{L}\prod_{i=1}^{L}s_i - \sum_{l=1}^{L}
      p_l\prod_{i=1}^{l-1} s_i
      \label{eq:rf_loc_recurrence_final_left}
      \end{align}\)
    </p>

    <p>
      This gives us the left-most feature position in the input image as a function of
      the padding (\(p_l\)) and stride (\(s_l\)) applied in each layer of the network,
      and of the feature location in the output feature map (\(u_{L}\)).
    </p>

    <p>
      And for the right-most feature location \(v_0\):
    </p>

    <p>
      \(\begin{align}
      v_0&amp;= v_{L}\prod_{i=1}^{L}s_i -\sum_{l=1}^{L}(1 + p_l -
      k_l)\prod_{i=1}^{l-1} s_i
      \label{eq:rf_loc_recurrence_final_right}
      \end{align}\)
    </p>

    <p>
      Note that, different from \eqref{eq:rf_loc_recurrence_final_left}, this
      expression also depends on the kernel sizes (\(k_l\)) of each layer.
    </p>

    <p>
      <strong>Relation between receptive field size and region.</strong>
      You may be wondering that
      the receptive field size \(r_0\) must be directly related to \(u_0\) and
      \(v_0\). Indeed, this is the case; it is easy to show that \(r_0 = v_0 - u_0 +
      1\), which we leave as a follow-up exercise for the curious reader. To
      emphasize, this means that we can rewrite
      \eqref{eq:rf_loc_recurrence_final_right} as:
    </p>

    <p>
      \(\begin{align}
      v_0&amp;= u_0 + r_0 - 1
      \label{eq:rf_loc_recurrence_final_right_rewrite}
      \end{align}\)
    </p>

    <p>
      <strong>Effective stride and effective padding.</strong>
      To compute \(u_0\) and \(v_0\) in practice, it
      is convenient to define two other variables, which depend only on the paddings
      and strides of the different layers:
    </p>

    <ul>
      <li>
        <em>effective stride</em>
        \(S_l = \prod_{i=l+1}^{L}s_i\): the stride between a
        given feature map \(f_l\) and the output feature map \(f_{L}\)
      </li>
      <li>
        <em>effective padding</em>
        \(P_l = \sum_{m=l+1}^{L}p_m\prod_{i=l+1}^{m-1} s_i\):
        the padding between a given feature map \(f_l\) and the output feature map
        \(f_{L}\)
      </li>
    </ul>

    <p>
      With these definitions, we can rewrite \eqref{eq:rf_loc_recurrence_final_left}
      as:
    </p>

    <p>
      \(\begin{align}
      u_0&amp;= -P_0 + u_{L}\cdot S_0
      \label{eq:rf_loc_recurrence_final_left_effective}
      \end{align}\)
    </p>

    <p>
      Note the resemblance between \eqref{eq:rf_loc_recurrence_final_left_effective}
      and \eqref{eq:rf_loc_recurrence_u}. By using \(S_l\) and \(P_l\), one can
      compute the locations \(u_l,v_l\) for feature map \(f_l\) given the location at
      the output feature map \(u_{L}\). When one is interested in computing feature
      locations for a given network, it is handy to pre-compute three variables:
      \(P_0,S_0,r_0\). Using these three, one can obtain \(u_0\) using
      \eqref{eq:rf_loc_recurrence_final_left_effective} and \(v_0\) using
      \eqref{eq:rf_loc_recurrence_final_right_rewrite}. This allows us to obtain the
      mapping from any output feature location to the input region which influences
      it.
    </p>

    <p>
      It is also possible to derive recurrence equations for the effective stride and
      effective padding. It is straightforward to show that:
    </p>

    <p>
      \(\begin{align}
      S_{l-1}&amp;= s_l \cdot S_l \label{eq:effective_stride_recurrence} \\
      P_{l-1}&amp;= s_l \cdot P_l + p_l \label{eq:effective_padding_recurrence}
      \end{align}\)
    </p>

    <p>
      These expressions will be handy when deriving an algorithm to solve the case
      for arbitrary computation graphs, presented in the next section.
    </p>

    <p>
      <strong>Center of receptive field region.</strong>
      It is also interesting to derive an
      expression for the center of the receptive field region which influences a
      particular output feature. This can be used as the location of the feature in
      the input image (as done for recent
      deep learning-based local features <d-cite key="noh2017large"></d-cite>, for
      example).
    </p>

    <p>
      We define the center of the receptive field region for each layer \(l\) as
      \(c_l = \frac{u_l + v_l}{2}\). Given the above expressions for \(u_0,v_0,r_0\),
      it is straightforward to derive \(c_0\) (remember that \(u_{L}=v_{L}\)):
    </p>

    <p>
      \(\begin{align}
      c_0&amp;= u_{L}\prod_{i=1}^{L}s_i
      - \sum_{l=1}^{L}
      \left(p_l - \frac{k_l - 1}{2}\right)\prod_{i=1}^{l-1} s_i \nonumber \\&amp;= u_{L}\cdot S_0
      - \sum_{l=1}^{L}
      \left(p_l - \frac{k_l - 1}{2}\right)\prod_{i=1}^{l-1} s_i
      \nonumber \\&amp;= -P_0 + u_{L}\cdot S_0 + \left(\frac{r_0 - 1}{2}\right)
      \label{eq:rf_loc_recurrence_final_center_effective}
      \end{align}\)
    </p>

    <p>
      This expression can be compared to
      \eqref{eq:rf_loc_recurrence_final_left_effective} to observe that the center is
      shifted from the left-most pixel by \(\frac{r_0 - 1}{2}\), which makes sense.
      Note that the receptive field centers for the different output features are
      spaced by the effective stride \(S_0\), as expected. Also, it is interesting to
      note that if \(p_l = \frac{k_l - 1}{2}\) for all \(l\), the centers of the
      receptive field regions for the output features will be aligned to the first
      image pixel and located at \({0, S_0, 2S_0, 3S_0, \ldots}\) (note that in this
      case all \(k_l\)'s must be odd).
    </p>

    <p>
      <strong>Other network operations.</strong>
      The derivations provided in this section cover most basic operations at the
      core of convolutional neural networks. A curious reader may be wondering
      about other commonly-used operations, such as dilation, upsampling, etc. You
      can find a discussion on these <a href="#other-network-operations" id="return-from-other-network-operations">in the appendix</a>.
    </p>

    <h2 id="final_heuristic_evaluation">Final Heuristic Evaluation</h2>

    <p>
      Most state-of-the-art convolutional neural networks today (e.g.,
      ResNet <d-cite key="he2016deep"></d-cite> or
      Inception <d-cite key="szegedy2016inception"></d-cite>) rely on models
      where each layer may have more than one input, which
      means that there might be several different paths from the input image to the
      final output feature map. These architectures are usually represented using
      directed acyclic computation graphs, where the set of nodes \(\mathcal{L}\)
      represents the layers and the set of edges \(\mathcal{E}\) encodes the
      connections between them (the feature maps flow through the edges).
    </p>

    <p>
      The computation presented in the previous section can be used for each of the
      possible paths from input to output independently. The situation becomes
      trickier when one wants to take into account all different paths to find the
      receptive field size of the network and the receptive field regions which
      correspond to each of the output features.
    </p>

    <p>
      <strong>Alignment issues.</strong>
      The first potential issue is that one output feature may
      be computed using misaligned regions of the input image, depending on the
      path from input to output. Also, the relative position between the image regions
      used for the computation of each output feature may vary. As a consequence,
      <strong>the receptive field size may not be shift-invariant</strong>
      . This is illustrated in the
      figure below with a toy example, in which case the centers of the regions used
      in the input image are different for the two paths from input to output.
    </p>

    <figure class="figure-contents" style="position: relative;">

      <img style="position: absolute; right: 0; top: -10px; width: 27px;" height="27" width="27" src="https://distill.pub/2018/feature-wise-transformations/images/pointer.svg" />
      
      <svg id="fig4_svg" height="450" width="800" style="position: relative; top: 0px;">
        Sorry, your browser does not support inline SVG.
      </svg>
      
      <div id="fig4_params_l1a" style="position: absolute; top: 53px; left: 0px; text-align: right;">
      <div style="height: 20px;">Kernel Size (k<sub>1</sub>): <span>5</span></div>
      <div style="height: 20px;">Left Pad (p<sub>1</sub>): <span>2</span></div>
      <div style="height: 20px;">Right Pad (q<sub>1</sub>): <span>1</span></div>
      <div style="height: 20px;">Stride (s<sub>1</sub>): <span>2</span></div>
      </div>
      <div id="fig4_params_l1b" style="position: absolute; top: 53px; right: 0px; text-align: right;">
      <div style="height: 20px;">Kernel Size (k<sub>2</sub>): <span>3</span></div>
      <div style="height: 20px;">Left Pad (p<sub>2</sub>): <span>0</span></div>
      <div style="height: 20px;">Right Pad (q<sub>2</sub>): <span>0</span></div>
      <div style="height: 20px;">Stride (s<sub>2</sub>): <span>1</span></div>
      </div>
      <div id="fig4_params_l2" style="position: absolute; top: 183px; right: 0px; text-align: right;">
      <div style="height: 20px;">Kernel Size (k<sub>3</sub>): <span>3</span></div>
      <div style="height: 20px;">Left Pad (p<sub>3</sub>): <span>0</span></div>
      <div style="height: 20px;">Right Pad (q<sub>3</sub>): <span>0</span></div>
      <div style="height: 20px;">Stride (s<sub>3</sub>): <span>1</span></div>
      </div>
      <div id="fig4_params_l3" style="position: absolute; top: 353px; right: 100px; text-align: right;">
      <div style="height: 20px;">Add</div>
      </div>
      
      <script type="text/javascript" src="assets/dynamic_visuals/fig4.js">
      </script>

    </figure>

    <p>
      In this example, padding is used only for the left branch. The first three layers
      are convolutional, while the last layer performs a simple addition.
      The relative position between the receptive field regions of the left and
      right paths is inconsistent for different output features, which leads to a
      lack of alignment (this can be seen by hovering over the different output features).
      Also, note that the receptive field size for each output
      feature may be different. For the second feature from the left, \(6\) input
      samples are used, while only \(5\) are used for the third feature. This means
      that the receptive field size may not be shift-invariant when the network is not
      aligned.
    </p>

    <p>
      For many computer vision tasks, it is highly desirable that output features be aligned:
      "image-to-image translation" tasks (e.g., semantic segmentation, edge detection,
      surface normal estimation, colorization, etc), local feature matching and
      retrieval, among others.
    </p>

    <p>
      When the network is aligned, all different paths lead to output features being
      centered consistently in the same locations. All different paths must have the
      same effective stride. It is easy to see that the receptive field size will be
      the largest receptive field among all possible paths. Also, the effective
      padding of the network corresponds to the effective padding for the path with
      largest receptive field size, such that one can apply
      \eqref{eq:rf_loc_recurrence_final_left_effective},
      \eqref{eq:rf_loc_recurrence_final_center_effective} to localize the region which
      generated an output feature.
    </p>

    <p>
      The figure below gives one simple example of an aligned network. In this case,
      the two different paths lead to the features being centered at the same
      locations. The receptive field size is \(3\), the effective stride is \(4\) and
      the effective padding is \(1\).
    </p>

    <figure class="figure-contents" style="position: relative;">

      <img style="position: absolute; right: 0; top: -10px; width: 27px;" height="27" width="27" src="https://distill.pub/2018/feature-wise-transformations/images/pointer.svg" />
      
      <svg id="fig5_svg" height="450" width="800" style="position: relative; top: 0px;">
        Sorry, your browser does not support inline SVG.
      </svg>
      
      <div id="fig5_params_l1a" style="position: absolute; top: 40px; left: 0px; text-align: right;">
      <div style="height: 20px;">Kernel Size (k<sub>1</sub>): <span>1</span></div>
      <div style="height: 20px;">Left Pad (p<sub>1</sub>): <span>0</span></div>
      <div style="height: 20px;">Right Pad (q<sub>1</sub>): <span>0</span></div>
      <div style="height: 20px;">Stride (s<sub>1</sub>): <span>4</span></div>
      </div>
      <div id="fig5_params_l1b" style="position: absolute; top: 40px; right: 0px; text-align: right;">
      <div style="height: 20px;">Kernel Size (k<sub>2</sub>): <span>3</span></div>
      <div style="height: 20px;">Left Pad (p<sub>2</sub>): <span>1</span></div>
      <div style="height: 20px;">Right Pad (q<sub>2</sub>): <span>0</span></div>
      <div style="height: 20px;">Stride (s<sub>2</sub>): <span>2</span></div>
      </div>
      <div id="fig5_params_l2" style="position: absolute; top: 170px; right: 0px; text-align: right;">
      <div style="height: 20px;">Kernel Size (k<sub>3</sub>): <span>1</span></div>
      <div style="height: 20px;">Left Pad (p<sub>3</sub>): <span>0</span></div>
      <div style="height: 20px;">Right Pad (q<sub>3</sub>): <span>0</span></div>
      <div style="height: 20px;">Stride (s<sub>3</sub>): <span>2</span></div>
      </div>
      <div id="fig5_params_l3" style="position: absolute; top: 340px; right: 100px; text-align: right;">
      <div style="height: 20px;">Add</div>
      </div>
      
      <script type="text/javascript" src="assets/dynamic_visuals/fig5.js">
      </script>

    </figure>

    <p>
      <strong>Alignment criteria</strong>
      . More precisely, for a network to be aligned at every
      layer, we need every possible pair of paths \(i\) and \(j\) to have
      \(c_l^{(i)} = c_l^{(j)}\) for any layer \(l\) and output feature \(u_{L}\). For
      this to happen, we can see from
      \eqref{eq:rf_loc_recurrence_final_center_effective} that two conditions must be
      satisfied:
    </p>

    <p>
      \(\begin{align}
      S_l^{(i)}&amp;= S_l^{(j)} \label{eq:align_crit_1} \\
      -P_l^{(i)} + \left(\frac{r_l^{(i)} - 1}{2}\right)&amp;= -P_l^{(j)} + \left(\frac{r_l^{(j)} - 1}{2}\right)
      \label{eq:align_crit_2}
      \end{align}\)
    </p>

    <p>for all \(i,j,l\).</p>

    <p>
      <strong>Algorithm for computing receptive field parameters: sketch.</strong>
      It is straightforward to develop an efficient algorithm that computes the receptive
      field size and associated parameters for such computation graphs.
      Naturally, a brute-force approach is to use the expressions presented above to
      compute the receptive field parameters for each route from the input to output independently,
      coupled with some bookkeeping in order to compute the parameters for the entire network.
      This method has a worst-case complexity of
      \(\mathcal{O}\left(\left|\mathcal{E}\right| \times \left|\mathcal{L}\right|\right)\).
    </p>
    <p>
      <!-- People may be tempted to apply DFS to this problem, which fails. To -->
      <!-- make the article more concise, we do not discuss this here. -->
      But we can do better. Start by topologically sorting the computation graph.
      The sorted representation arranges the layers in order of dependence: each
      layer's output only depends on layers that appear before it.
      By visiting layers in reverse topological order, we ensure that all paths
      from a given layer \(l\) to the output layer \(L\) have been taken into account
      when \(l\) is visited. Once the input layer \(l=0\) is reached, all paths
      have been considered and the receptive field parameters of the entire model
      are obtained. The complexity of this algorithm is
      \(\mathcal{O}\left(\left|\mathcal{E}\right| + \left|\mathcal{L}\right|\right)\),
      which is much better than the brute-force alternative.
    </p>
    <p>
      As each layer is visited, some bookkeeping must be done in order to keep
      track of the network's receptive field parameters. In particular, note that
      there might be several different paths from layer \(l\) to the output layer
      \(L\). In order to handle this situation, we keep track of the parameters
      for \(l\) and update them if a new path with larger receptive field is found,
      using expressions \eqref{eq:rf_recurrence}, \eqref{eq:effective_stride_recurrence}
      and \eqref{eq:effective_padding_recurrence}.
      Similarly, as the graph is traversed, it is important to check that the network is aligned.
      This can be done by making sure that the receptive field parameters of different paths satisfy
      \eqref{eq:align_crit_1} and \eqref{eq:align_crit_2}.
    </p>

    <h2 id="discussion">Discussion: Recommendations to Improve the System</h2>

    <p>
      In this section, we present the receptive field parameters of modern
      convolutional networks
      <d-footnote>
        The models used for receptive field computations, as well as the accuracy reported on ImageNet experiments,
        are drawn from the <a href="https://github.com/tensorflow/models/tree/master/research/slim">TF-Slim
          image classification model library</a>.
      </d-footnote>
      , which were computed using the new open-source
      library (script
      <a
        href="https://github.com/google-research/receptive_field/blob/master/receptive_field/python/util/examples/rf_benchmark.py">here</a>).
      The pre-computed parameters for
      AlexNet <d-cite key="krizhevsky2012imagenet"></d-cite>,
      VGG <d-cite key="simonyan2015very"></d-cite>,
      ResNet <d-cite key="he2016deep"></d-cite>,
      Inception <d-cite key="szegedy2016inception"></d-cite>
      and
      MobileNet <d-cite key="howard2017mobilenets"></d-cite>
      are presented in the table below.
      For a more
      comprehensive list, including intermediate network end-points, see
      <a
        href="https://github.com/google-research/receptive_field/blob/master/receptive_field/RECEPTIVE_FIELD_TABLE.md">this
        table</a>.
    </p>

    <p>
      <table class="model_table" style="margin: 0px auto;">
        <thead>
          <tr>
            <th align="center">ConvNet Model</th>
            <th align="center">Receptive <br />Field (r)</th>
            <th align="center">Effective <br />Stride (S)</th>
            <th align="center">Effective <br />Padding (P)</th>
            <th align="center">Model Year</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="center">alexnet_v2</td>
            <td align="center">195</td>
            <td align="center">32</td>
            <td align="center">64</td>
            <td align="center"><a href="https://arxiv.org/abs/1404.5997v2" target="_new">2014</a></td>
          </tr>
          <tr>
            <td align="center">vgg_16</td>
            <td align="center">212</td>
            <td align="center">32</td>
            <td align="center">90</td>
            <td align="center"><a href="https://arxiv.org/abs/1409.1556" target="_new">2014</a></td>
          </tr>
          <tr>
            <td align="center">mobilenet_v1</td>
            <td align="center">315</td>
            <td align="center">32</td>
            <td align="center">126</td>
            <td align="center"><a href="https://arxiv.org/abs/1704.04861" target="_new">2017</a></td>
          </tr>
          <tr>
            <td align="center">mobilenet_v1_075</td>
            <td align="center">315</td>
            <td align="center">32</td>
            <td align="center">126</td>
            <td align="center"><a href="https://arxiv.org/abs/1704.04861" target="_new">2017</a></td>
          </tr>
          <tr>
            <td align="center">resnet_v1_50</td>
            <td align="center">483</td>
            <td align="center">32</td>
            <td align="center">239</td>
            <td align="center"><a href="https://arxiv.org/abs/1512.03385" target="_new">2015</a></td>
          </tr>
          <tr>
            <td align="center">inception_v2</td>
            <td align="center">699</td>
            <td align="center">32</td>
            <td align="center">318</td>
            <td align="center"><a href="https://arxiv.org/abs/1502.03167" target="_new">2015</a></td>
          </tr>
          <tr>
            <td align="center">resnet_v1_101</td>
            <td align="center">1027</td>
            <td align="center">32</td>
            <td align="center">511</td>
            <td align="center"><a href="https://arxiv.org/abs/1512.03385" target="_new">2015</a></td>
          </tr>
          <tr>
            <td align="center">inception_v3</td>
            <td align="center">1311</td>
            <td align="center">32</td>
            <td align="center">618</td>
            <td align="center"><a href="https://arxiv.org/abs/1512.00567" target="_new">2015</a></td>
          </tr>
          <tr>
            <td align="center">resnet_v1_152</td>
            <td align="center">1507</td>
            <td align="center">32</td>
            <td align="center">751</td>
            <td align="center"><a href="https://arxiv.org/abs/1512.03385" target="_new">2015</a></td>
          </tr>
          <tr>
            <td align="center">resnet_v1_200</td>
            <td align="center">1763</td>
            <td align="center">32</td>
            <td align="center">879</td>
            <td align="center"><a href="https://arxiv.org/abs/1512.03385" target="_new">2015</a></td>
          </tr>
          <tr>
            <td align="center">inception_v4</td>
            <td align="center">2071</td>
            <td align="center">32</td>
            <td align="center">998</td>
            <td align="center"><a href="https://arxiv.org/abs/1602.07261" target="_new">2016</a></td>
          </tr>
          <tr>
            <td align="center">inception_resnet_v2</td>
            <td align="center">3039</td>
            <td align="center">32</td>
            <td align="center">1482</td>
            <td align="center"><a href="https://arxiv.org/abs/1602.07261" target="_new">2016</a></td>
          </tr>
        </tbody>
      </table>
    </p>

    <p>
      As models evolved, from
      AlexNet, to VGG, to ResNet and Inception, the receptive fields increased
      (which is a natural consequence of the increased number of layers).
      In the most recent networks, the receptive field usually covers the entire input image:
      this means that the context used by each feature in the final output feature map
      includes all of the input pixels.
    </p>
    <p>
      We can also relate the growth in receptive fields to increased
      classification accuracy. The figure below plots ImageNet
      top-1 accuracy as a function of the network's receptive field size, for
      the same networks listed above. The circle size for each data point is
      proportional to the number of floating-point operations (FLOPs) for each
      architecture.
    </p>

    <p>
      <script type="text/javascript" src="https://www.gstatic.com/charts/loader.js"></script>
      <script type="text/javascript">
        google.charts.load('current', { 'packages': ['corechart'] });
        google.charts.setOnLoadCallback(drawSeriesChart);

        function drawSeriesChart() {

          var data = google.visualization.arrayToDataTable([
            ['ID', 'Receptive field size (pixels)', 'ImageNet top-1 accuracy', 'Family', 'FLOPS (Billion)'],
            ['alexnet_v2', 195, 0.5720, 'alexnet', 1.38],
            ['vgg_16', 212, 0.7150, 'vgg_16', 30.71],
            ['inception_v2', 699, 0.7390, 'inception', 3.88],
            ['inception_v3', 1311, 0.7800, 'inception', 5.69],
            ['inception_v4', 2071, 0.8020, 'inception', 12.27],
            ['inception_resnet_v2', 3039, 0.8040, 'inception_resnet', 12.96],
            ['resnet_v1_50', 483, 0.7520, 'resnet', 6.97],
            ['resnet_v1_101', 1027, 0.7640, 'resnet', 14.40],
            ['resnet_v1_152', 1507, 0.7680, 'resnet', 21.82],
            ['mobilenet_v1', 315, 0.7090, 'mobilenet', 1.14]
          ]);

          var options = {
            title: '',
            hAxis: { title: 'Receptive field size (pixels)', gridlines: { count: 10 } },
            vAxis: { title: 'ImageNet top-1 accuracy', format: 'percent' },
            sizeAxis: { maxSize: 10 },
            legend: { position: 'bottom', textStyle: { fontSize: 12 } },
            bubble: { textStyle: { fontSize: 11 } }
          };

          var chart = new google.visualization.BubbleChart(document.getElementById('series_chart_div'));
          chart.draw(data, options);
        }
      </script>
      <div style="width: 100%; max-width: 680px; height: 500px; position: relative;">
        <div id="series_chart_div" style="position: relative; width: 100%; height: 500px;"></div>
      </div>
    </p>

    <p>
      We observe a logarithmic relationship between
      classification accuracy and receptive field size, which suggests
      that large receptive fields are necessary for high-level
      recognition tasks, but with diminishing rewards.
      For example, note how MobileNets achieve high recognition performance even
      if using a very compact architecture: with depth-wise convolutions,
      the receptive field is increased with a small compute footprint.
      In comparison, VGG-16 requires 27X more FLOPs than MobileNets, but produces
      a smaller receptive field size; even if much more complex, VGG's accuracy
      is only slightly better than MobileNet's.
      This suggests that networks which can efficiently generate large receptive
      fields may enjoy enhanced recognition performance.
    </p>
    <p>
      Let us emphasize, though, that the receptive field size is not the only factor contributing
      to the improved performance mentioned above. Other factors play a very important
      role: network depth (i.e., number of layers) and width (i.e., number of filters per layer),
      residual connections, batch normalization, to name only a few.
      In other words, while we conjecture that a large receptive field is necessary,
      by no means it is sufficient.
      <d-footnote>
        Additional experimentation is needed to confirm this hypothesis: for
        example, researchers may experimentally investigate how classification
        accuracy changes as kernel sizes and strides vary for different
        architectures. This may indicate if, at least for those architectures, a
        large receptive field is necessary.
      </d-footnote>
    </p>
    <p>
      Finally, note that a given feature is not equally impacted by all input pixels within
      its receptive field region: the input pixels near the center of the receptive field have more "paths" to influence
      the feature, and consequently carry more weight.
      The relative importance of each input pixel defines the
      <i>effective receptive field</i> of the feature.
      Recent work <d-cite key="luo2016understanding"></d-cite>
      provides a mathematical formulation and a procedure to measure effective
      receptive fields, experimentally observing a Gaussian shape,
      with the peak at the receptive field center. Better understanding the
      relative importance of input pixels in convolutional neural networks is
      an active research topic.
    </p>

  </d-article>

  <d-appendix>
    <h3 id="solving-receptive-field-size">Solving recurrence equations: receptive field size</h3>
    <p>
      The first trick to solve
      \eqref{eq:rf_recurrence} is to multiply it by \(\prod_{i=1}^{l-1} s_i\):
    </p>

    <p>
      \(\begin{align}
      r_{l-1}\prod_{i=1}^{l-1} s_i&amp; = s_l \cdot r_{l}\prod_{i=1}^{l-1} s_i + (k_l - s_l)\prod_{i=1}^{l-1} s_i
      \nonumber \\&amp; = r_{l}\prod_{i=1}^{l} s_i + k_l\prod_{i=1}^{l-1} s_i - \prod_{i=1}^{l} s_i
      \label{eq:rf_recurrence_mult}\
      \end{align}\)
    </p>

    <p>
      Then, define \(A_l = r_l\prod_{i=1}^{l}s_i\), and note that
      \(\prod_{i=1}^{0}s_i = 1\) (since \(1\) is the neutral element for
      multiplication), so \(A_0 = r_0\). Using this definition,
      \eqref{eq:rf_recurrence_mult} can be rewritten as:
    </p>

    <p>
      \begin{equation}
      A_{l} - A_{l-1} = \prod_{i=1}^{l} s_i - k_l\prod_{i=1}^{l-1} s_i
      \label{eq:rf_recurrence_adef}
      \end{equation}
    </p>

    <p>Now, sum it from \(l=1\) to \(l=L\):</p>

    <p>
      \(\begin{align}
      \sum_{l=1}^{L} \left(A_{l} - A_{l-1} \right) = A_{L} - A_0 = \sum_{l=1}^{L}
      \left(\prod_{i=1}^{l} s_i - k_l\prod_{i=1}^{l-1} s_i \right)
      \label{eq:rf_recurrence_sum_a}
      \end{align}\)
    </p>

    <p>
      Note that \(A_0 = r_0\) and \(A_{L} = r_{L}\prod_{i=1}^{L}s_i =
      \prod_{i=1}^{L}s_i\). Thus, we can compute:
    </p>

    <p>
      \(\begin{align}
      r_0&amp;= \prod_{i=1}^{L}s_i + \sum_{l=1}^{L} \left(k_l\prod_{i=1}^{l-1} s_i
      - \prod_{i=1}^{l} s_i \right) \nonumber \\&amp;= \sum_{l=1}^{L}k_l\prod_{i=1}^{l-1}
      s_i-\sum_{l=1}^{L-1}\prod_{i=1}^{l} s_i
      \nonumber \\&amp;= \sum_{l=1}^{L}k_l\prod_{i=1}^{l-1} s_i-\sum_{l=1}^{L}\prod_{i=1}^{l-1}s_i
      + 1 \label{eq:rf_recurrence_almost_final}
      \end{align}\)
    </p>

    <p>where the last step is done by a change of variables for the right term.</p>

    <p>
      Finally, rewriting \eqref{eq:rf_recurrence_almost_final}, we obtain the
      expression for the receptive field size \(r_0\) of an FCN at the input image,
      given the parameters of each layer:
    </p>

    <p>
      \begin{equation}
      r_0 = \sum_{l=1}^{L} \left((k_l-1)\prod_{i=1}^{l-1} s_i\right) + 1
      \end{equation}
    </p>

    <p><a href="#return-from-solving-receptive-field-size">Navigate back to the main text</a><p>

    <h3 id="solving-receptive-field-region">Solving recurrence equations: receptive field region</h3>
    <p>
      The derivations are similar to the one we use
      to solve \eqref{eq:rf_recurrence}. Let's consider the computation of \(u_0\).
      First, multiply \eqref{eq:rf_loc_recurrence_u} by \(\prod_{i=1}^{l-1} s_i\).
    </p>

    <p>
      \(\begin{align}
      u_{l-1}\prod_{i=1}^{l-1} s_i&amp; = u_{l} \cdot s_l\prod_{i=1}^{l-1} s_i - p_l\prod_{i=1}^{l-1} s_i\nonumber
      \\&amp; = u_{l}\prod_{i=1}^{l} s_i - p_l\prod_{i=1}^{l-1} s_i
      \label{eq:rf_loc_recurrence_mult}\
      \end{align}\)
    </p>

    <p>
      Then, define \(B_l = u_l\prod_{i=1}^{l}s_i\), and rewrite
      \eqref{eq:rf_loc_recurrence_mult} as:
    </p>

    <p>
      \begin{equation}
      B_{l} - B_{l-1} = p_l\prod_{i=1}^{l-1} s_i
      \label{eq:rf_loc_recurrence_adef}
      \end{equation}
    </p>

    <p>And sum it from \(l=1\) to \(l=L\):</p>

    <p>
      \(\begin{align}
      \sum_{l=1}^{L} \left(B_{l} - B_{l-1} \right) = B_{L} - B_0 =
      \sum_{l=1}^{L} p_l\prod_{i=1}^{l-1} s_i \label{eq:rf_loc_recurrence_sum_a}\
      \end{align}\)
    </p>

    <p>
      Note that \(B_0 = u_0\) and \(B_{L} = u_{L}\prod_{i=1}^{L}s_i\). Thus, we can
      compute:
    </p>

    <p>
      \(\begin{align}
      u_0&amp;= u_{L}\prod_{i=1}^{L}s_i - \sum_{l=1}^{L}
      p_l\prod_{i=1}^{l-1} s_i
      \end{align}\)
    </p>

    <p><a href="#return-from-solving-receptive-field-region">Navigate back to the main text</a><p>

    <h3 id="other-network-operations">Other network operations</h3>
    <p>
      <strong>Dilated (atrous) convolution.</strong>
      Dilations introduce "holes" in a convolutional kernel. While the number of
      weights in the kernel is unchanged, they are no longer applied to spatially
      adjacent
      samples. Dilating a kernel by a factor of \(\alpha\) introduces striding of
      \(\alpha\) between the samples used when computing the convolution. This
      means that the spatial span of the kernel (\(k>0\)) is increased to \(\alpha (k-1) + 1\).
      The above derivations can be reused by simply replacing the kernel size
      \(k\) by \(\alpha (k-1) + 1\) for all layers using dilations.
    </p>

    <p>
      <strong>Upsampling.</strong>
      Upsampling is frequently done using interpolation (e.g., bilinear, bicubic
      or nearest-neighbor methods), resulting in an equal or larger receptive
      field &mdash; since it relies on one or multiple features from the input.
      Upsampling layers generally produce output features which depend locally on
      input features, and for receptive field computation purposes can be
      considered to have a kernel size equal to the number of input features
      involved in the computation of an output feature.
    </p>

    <p>
      <strong>Separable convolutions.</strong>
      Convolutions may be separable in terms of spatial or channel dimensions.
      The receptive field properties of the separable convolution are
      identical to its corresponding equivalent non-separable convolution. For
      example, a \(3 \times 3\) depth-wise separable convolution has a kernel
      size of \(3\) for receptive field computation purposes.
    </p>

    <p>
      <strong>Batch normalization.</strong>
      At inference time, batch normalization consists of feature-wise operations
      which do not alter the receptive field of the network. During training,
      however, batch normalization parameters are computed based on all
      activations from a specific layer, which means that its receptive field is
      the whole input image.
    </p>

    <p><a href="#return-from-other-network-operations">Navigate back to the main text</a><p>

    <h3>Acknowledgments</h3>
    <p>
      We would like to thank Yuning Chai and George Papandreou for their careful
      review of early drafts of this manuscript.
      Regarding the open-source library, we thank Mark Sandler for helping with
      the starter code, Liang-Chieh Chen and Iaroslav Tymchenko for careful
      code review, and Till Hoffman for improving upon the original code release.
      Thanks also to Mark Sandler for assistance with model profiling.
    </p>

    <d-footnote-list></d-footnote-list>
    <d-bibliography src="bibliography.bib"></d-bibliography>

  </d-appendix>

</body>
